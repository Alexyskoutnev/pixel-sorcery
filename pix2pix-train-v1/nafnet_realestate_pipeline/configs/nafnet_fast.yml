# ============================================================
# NAFNet Fast Training Config - Real Estate Photo Enhancement
# Optimized for ~4-5 hours on GB10 (128GB) or ~6-8 hours on A100 (40GB)
# ============================================================
# 
# Key optimizations for speed:
#   - width: 32 (smaller model, still good quality)
#   - gt_size: 384 (balanced patch size)
#   - batch_size: 12 (utilizes GB10 memory)
#   - total_iter: 30000 (sufficient for 500+ pairs)
#   - Transfer learning from pretrained weights
#
# Expected results: ~80% of full training quality
# PSNR improvement: +2-3 dB over input images
# ============================================================

name: NAFNet_RealEstate_Fast
model_type: ImageRestorationModel
scale: 1
num_gpu: 1
manual_seed: 42

# ============================================================
# Dataset Configuration
# ============================================================
datasets:
  train:
    name: RealEstate_Train
    type: PairedImageDataset
    phase: train

    # Paths to your prepared dataset (absolute paths)
    dataroot_gt: /home/asus/sebastian/pixel-sorcery/pix2pix-train-v1/datasets/realestate/train/gt
    dataroot_lq: /home/asus/sebastian/pixel-sorcery/pix2pix-train-v1/datasets/realestate/train/lq

    # Scale factor (1 = no upscaling, image enhancement only)
    scale: 1

    # File handling
    filename_tmpl: '{}'
    io_backend:
      type: disk
    
    # Patch size - 256 for faster training
    gt_size: 256
    
    # Augmentation - essential for 500 image dataset
    use_hflip: true                 # Horizontal flip (safe for real estate)
    use_rot: true                   # 90° rotations
    
    # DataLoader settings for GB10
    num_worker_per_gpu: 6
    batch_size_per_gpu: 20          # Increased for faster training at 256px
    
    # CRITICAL for small datasets: repeat data many times per epoch
    dataset_enlarge_ratio: 50       # 500 images × 50 = 25000 per epoch
    
    prefetch_mode: ~

  val:
    name: RealEstate_Val
    type: PairedImageDataset
    phase: val
    dataroot_gt: /home/asus/sebastian/pixel-sorcery/pix2pix-train-v1/datasets/realestate/val/gt
    dataroot_lq: /home/asus/sebastian/pixel-sorcery/pix2pix-train-v1/datasets/realestate/val/lq
    scale: 1
    io_backend:
      type: disk

# ============================================================
# Network Architecture - NAFNet width-32 (fast + good quality)
# ============================================================
network_g:
  type: NAFNet
  img_channel: 3
  width: 32                         # 32 for speed, 64 for max quality
  middle_blk_num: 12
  enc_blk_nums: [2, 2, 4, 8]
  dec_blk_nums: [2, 2, 2, 2]

# ============================================================
# Path Configuration
# ============================================================
path:
  # Using pretrained weights for transfer learning (recommended per whitepaper)
  pretrain_network_g: /home/asus/sebastian/pixel-sorcery/pix2pix-train-v1/pretrained/NAFNet-SIDD-width32.pth
  strict_load_g: true  # Architecture matches exactly (width=32)
  resume_state: /home/asus/sebastian/pixel-sorcery/pix2pix-train-v1/BasicSR/experiments/NAFNet_RealEstate_Fast/training_states/4000.state

# ============================================================
# Training Configuration
# ============================================================
train:
  # Exponential moving average for stable inference
  ema_decay: 0.999
  
  # Optimizer - AdamW works best for transformers
  optim_g:
    type: AdamW
    lr: !!float 1e-3                # 1e-3 for training from scratch
    weight_decay: !!float 1e-4      # Regularization
    betas: [0.9, 0.9]
  
  # Learning rate schedule - cosine annealing with restarts (per whitepaper)
  scheduler:
    type: CosineAnnealingRestartLR
    periods: [12000]                # Match total_iter
    restart_weights: [1]
    eta_min: !!float 1e-7           # Minimum LR
  
  # Total training iterations
  total_iter: 12000                 # ~4-5 hours on GB10
  warmup_iter: 200                  # Warmup for stability
  
  # --------------------------------------------------------
  # Loss Functions
  # --------------------------------------------------------
  
  # L1 Pixel Loss (primary) - preserves brightness/color fidelity
  pixel_opt:
    type: L1Loss
    loss_weight: 1.0
    reduction: mean
  
  # Perceptual Loss (secondary) - improves visual quality
  # Per whitepaper: "Starting weights of λ_perc = 0.1 provide good results"
  perceptual_opt:
    type: PerceptualLoss
    layer_weights:
      'conv3_4': 0.5
      'conv4_4': 0.5
      'conv5_4': 1.0
    vgg_type: vgg19
    use_input_norm: true
    perceptual_weight: 0.1          # Per whitepaper recommendation (0.01-0.5 range)
    style_weight: 0.0               # Disable style loss
    criterion: l1

# ============================================================
# Validation Configuration
# ============================================================
val:
  val_freq: !!float 2000            # Validate every 2000 iterations
  save_img: true                    # Save sample outputs
  
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 4
      test_y_channel: false         # Use RGB, not just luminance
    
    ssim:
      type: calculate_ssim
      crop_border: 4
      test_y_channel: false

# ============================================================
# Logging Configuration
# ============================================================
logger:
  print_freq: 100                   # Print every 100 iterations
  save_checkpoint_freq: !!float 2000  # Save model every 2000 iterations
  use_tb_logger: true               # Enable TensorBoard
  wandb:
    project: ~                      # Set to enable W&B logging
    resume_id: ~

# ============================================================
# Distributed Training (single GPU)
# ============================================================
dist_params:
  backend: nccl
  port: 29500
